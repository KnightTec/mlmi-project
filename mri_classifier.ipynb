{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load MIRIAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([1, 1, 37, 77, 77])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from miriad import MiriadDataset\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "train_dataset = MiriadDataset(root=\"data/miriad\", train=True, transform=transform, crop=0.7)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "val_dataset = MiriadDataset(root=\"data/miriad\", train=False, transform=transform, crop=0.7)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('HC', 'AD')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "mris, labels = next(dataiter)\n",
    "\n",
    "print(mris.size())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "i = 0\n",
    "for sample_data, sample_label in zip(mris, labels):\n",
    "    mri_sample_slice = sample_data[0, 16, :, :]\n",
    "    plt.subplot(1, batch_size, i+1)  # Create a subplot for each sample\n",
    "    plt.imshow(mri_sample_slice.T, cmap='gray', origin='lower')\n",
    "    plt.title(f\"{classes[sample_label]}\")  # Display the label as the title\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 3D-CNN Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlzheimersClassification3DCNN(\n",
      "  (_conv_layers): Sequential(\n",
      "    (0): Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): AdaptiveAvgPool3d(output_size=(4, 4, 4))\n",
      "  )\n",
      "  (_fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (_fc2): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Parameter count = 268153\n"
     ]
    }
   ],
   "source": [
    "from models import AlzheimersClassification3DCNN\n",
    "\n",
    "model = AlzheimersClassification3DCNN().double().to(device)\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameter count = {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e30327b01794a159f5d524f79f459a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[INFO]: Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     train_epoch_loss, train_epoch_acc \u001b[39m=\u001b[39m train(model, train_loader, \n\u001b[1;32m     17\u001b[0m                                             optimizer, criterion, device)\n\u001b[1;32m     18\u001b[0m     valid_epoch_loss, valid_epoch_acc \u001b[39m=\u001b[39m validate(model, val_loader,  \n\u001b[1;32m     19\u001b[0m                                                 criterion, device)\n\u001b[1;32m     20\u001b[0m     writer\u001b[39m.\u001b[39madd_scalars(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39mfloat\u001b[39m(train_epoch_loss), \u001b[39m\"\u001b[39m\u001b[39mValidation\u001b[39m\u001b[39m\"\u001b[39m: valid_epoch_loss}, epoch)\n",
      "File \u001b[0;32m~/workspace/mlmi-project/training.py:21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[39m=\u001b[39m model(image)\n\u001b[1;32m     22\u001b[0m \u001b[39m# calculate the loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/mlmi-project/models.py:36\u001b[0m, in \u001b[0;36mAlzheimersClassification3DCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 36\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_layers(x)\n\u001b[1;32m     37\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m) \u001b[39m# flatten all dimensions except batch\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fc1(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(\n\u001b[1;32m    598\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    599\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\n\u001b[1;32m    609\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    610\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from training import train, validate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# training \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0002, momentum=0.9, weight_decay=0.001)\n",
    "epochs = 25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n",
    "                                            optimizer, criterion, device)\n",
    "    valid_epoch_loss, valid_epoch_acc = validate(model, val_loader,  \n",
    "                                                criterion, device)\n",
    "    writer.add_scalars(\"Loss\", {\"Training\" : float(train_epoch_loss), \"Validation\": valid_epoch_loss}, epoch)\n",
    "    writer.add_scalars(\"Accuracy\", {\"Training\" : train_epoch_acc, \"Validation\": valid_epoch_acc}, epoch)\n",
    "    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Swin-Transformer Classification Model (scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SvinViTAlzheimersClassifier(\n",
      "  (_svin_vit): SwinTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv3d(1, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers1): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=48, out_features=48, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLPBlock(\n",
      "              (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
      "              (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
      "              (fn): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layers2): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLPBlock(\n",
      "              (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (fn): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layers3): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLPBlock(\n",
      "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (fn): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
      "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layers4): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLPBlock(\n",
      "              (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (fn): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=3072, out_features=768, bias=False)\n",
      "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_global_avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "  (_fc): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n",
      "Parameter count = 8062771\n"
     ]
    }
   ],
   "source": [
    "from models import SvinViTAlzheimersClassifier\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "swin_model = SvinViTAlzheimersClassifier().double().to(device)\n",
    "print(swin_model)\n",
    "total_params = sum(p.numel() for p in swin_model.parameters())\n",
    "print(f\"Parameter count = {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f9c382b8594989bc377a7b4dfe354c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcedb58e8c14d5ca16ce4f4b80b4e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.793, training acc: 52.500\n",
      "Validation loss: 0.766, validation acc: 58.750\n",
      "[INFO]: Epoch 2 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26026882177249a584bbbe4ff93cd805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b48ba460414281840835c359a51634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.738, training acc: 48.125\n",
      "Validation loss: 0.775, validation acc: 41.250\n",
      "[INFO]: Epoch 3 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e453806f5b44d4c90d49d60299f87fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc3bbb09fdf4807a7f679592f506984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.713, training acc: 50.000\n",
      "Validation loss: 0.686, validation acc: 58.750\n",
      "[INFO]: Epoch 4 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c23d8555d542a8a6c75827645342e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8658cc5a5c284b71a1db5d7769aaab9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.712, training acc: 50.625\n",
      "Validation loss: 0.746, validation acc: 41.250\n",
      "[INFO]: Epoch 5 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6476a64610c64234aa67be8dfee392dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46d1bc98da54e83b5a39b645299fd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.714, training acc: 48.125\n",
      "Validation loss: 0.678, validation acc: 41.250\n",
      "[INFO]: Epoch 6 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394b8be05236453ba73f1a9821cb78bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5aaa389341e4bc49f248e74e90500c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.694, training acc: 53.125\n",
      "Validation loss: 0.819, validation acc: 58.750\n",
      "[INFO]: Epoch 7 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120ffe064d324b859c2d8b2ad05fb48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3ce67a61fa430c911af3972dcdbed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.726, training acc: 51.250\n",
      "Validation loss: 0.794, validation acc: 41.250\n",
      "[INFO]: Epoch 8 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb82d1967c7a41d2a9d2681cc38093f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bcb0bc77184743a42890632b94bd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.702, training acc: 50.000\n",
      "Validation loss: 0.678, validation acc: 41.250\n",
      "[INFO]: Epoch 9 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f418413e50e4b89bf4b60cf168c52ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ec26ba0af49e09df0b02d6c27193b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.707, training acc: 50.000\n",
      "Validation loss: 0.684, validation acc: 41.250\n",
      "[INFO]: Epoch 10 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a9a49116444ecfa6f4ab2d2db21a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0429075f3b4aea86fc6bcf33f74e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.706, training acc: 49.375\n",
      "Validation loss: 0.682, validation acc: 58.750\n",
      "[INFO]: Epoch 11 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d5f9065a574d3e9fea7dd4ab0f5f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb02b92e5e314a968dce8b4f72b5d7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.714, training acc: 48.125\n",
      "Validation loss: 0.705, validation acc: 41.250\n",
      "[INFO]: Epoch 12 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d576a6958a0c4fc9a160227d5c491fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f470d98a48ee48139b1f065966856076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.700, training acc: 50.625\n",
      "Validation loss: 0.678, validation acc: 41.250\n",
      "[INFO]: Epoch 13 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc30e696acd4908b9c1fa09ba521d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fbde5f03f34affa2873c652a32a8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.699, training acc: 54.375\n",
      "Validation loss: 0.763, validation acc: 41.250\n",
      "[INFO]: Epoch 14 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0facb95f46466c92cb10947975fafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b581c75fa443eaa3c648a55f8904e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.703, training acc: 50.000\n",
      "Validation loss: 0.680, validation acc: 41.250\n",
      "[INFO]: Epoch 15 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ff8ee29dd149e5bbe34818d0007a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b83821f8024699a03e030d6f55e067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.696, training acc: 50.000\n",
      "Validation loss: 0.681, validation acc: 41.250\n",
      "[INFO]: Epoch 16 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ba38255be04e41936b64a039d1ae43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7308a4e5286c41ec86158d78af473423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.699, training acc: 50.000\n",
      "Validation loss: 0.687, validation acc: 41.250\n",
      "[INFO]: Epoch 17 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2076fe0688ea498c8ebe45f9ac6a708d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27acfb7b72184d9baf2131f0ae22f7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.710, training acc: 50.000\n",
      "Validation loss: 0.678, validation acc: 41.250\n",
      "[INFO]: Epoch 18 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110f928b66024c31b772dc214d34936f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f3a6b4dada4b5186da62c70088456f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.699, training acc: 52.500\n",
      "Validation loss: 0.713, validation acc: 41.250\n",
      "[INFO]: Epoch 19 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8e598d963c4d14a41f576e9c48976c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1450d6743d2f4d06b0b6896ad8c50054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.707, training acc: 50.000\n",
      "Validation loss: 0.710, validation acc: 41.250\n",
      "[INFO]: Epoch 20 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb5b2a3608f4295a722b5156558ab0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6e37477c8b445f8534ca79fea3e4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.702, training acc: 50.000\n",
      "Validation loss: 0.721, validation acc: 41.250\n",
      "[INFO]: Epoch 21 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99a914aefa444cd9e3967d9d03dd39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d128eaff2ca4ef59a7c8421152e2d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.700, training acc: 50.000\n",
      "Validation loss: 0.688, validation acc: 41.250\n",
      "[INFO]: Epoch 22 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41ff35d92d245c49aab24df9090566a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14859ac69394cf1bd004f85af16d158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.699, training acc: 50.000\n",
      "Validation loss: 0.680, validation acc: 41.250\n",
      "[INFO]: Epoch 23 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88dc43f6bdef4e889c44f7a31c6dda5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df836cee235b47a6903daa60f31e3a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.700, training acc: 50.000\n",
      "Validation loss: 0.688, validation acc: 41.250\n",
      "[INFO]: Epoch 24 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bc954b0c9d48b390115659212b9643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a540be76a05645079d3a85e3e1b07c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.702, training acc: 50.000\n",
      "Validation loss: 0.697, validation acc: 41.250\n",
      "[INFO]: Epoch 25 of 25\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309de978574d4f94a7b0359194fc86b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c44e563eba94c2db7766e72f3ab22da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.698, training acc: 50.000\n",
      "Validation loss: 0.698, validation acc: 41.250\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from training import train, validate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# training \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(swin_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "epochs = 25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(swin_model, train_loader, \n",
    "                                            optimizer, criterion, device)\n",
    "    valid_epoch_loss, valid_epoch_acc = validate(swin_model, val_loader,  \n",
    "                                                criterion, device)\n",
    "    writer.add_scalars(\"Loss\", {\"Training\" : float(train_epoch_loss), \"Validation\": valid_epoch_loss}, epoch)\n",
    "    writer.add_scalars(\"Accuracy\", {\"Training\" : train_epoch_acc, \"Validation\": valid_epoch_acc}, epoch)\n",
    "    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Swin-Transformer Classification Small Model (scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SvinViTAlzheimersClassifierSmall(\n",
      "  (_svin_vit): SwinTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv3d(1, 8, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers1): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=8, out_features=24, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=8, out_features=8, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLPBlock(\n",
      "              (linear1): Linear(in_features=8, out_features=16, bias=True)\n",
      "              (linear2): Linear(in_features=16, out_features=8, bias=True)\n",
      "              (fn): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=64, out_features=16, bias=False)\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layers2): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=16, out_features=48, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLPBlock(\n",
      "              (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
      "              (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
      "              (fn): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=128, out_features=32, bias=False)\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layers3): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLPBlock(\n",
      "              (linear1): Linear(in_features=32, out_features=64, bias=True)\n",
      "              (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
      "              (fn): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=256, out_features=64, bias=False)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layers4): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLPBlock(\n",
      "              (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "              (fn): GELU(approximate='none')\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=512, out_features=128, bias=False)\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (_global_avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "  (_fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Parameter count = 222581\n"
     ]
    }
   ],
   "source": [
    "from models import SvinViTAlzheimersClassifierSmall\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "swin_model_small = SvinViTAlzheimersClassifierSmall().double().to(device)\n",
    "print(swin_model_small)\n",
    "total_params = sum(p.numel() for p in swin_model_small.parameters())\n",
    "print(f\"Parameter count = {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 30\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70485b1bbce341b9af8aed975d6b559f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [1, 36, 77, 77] at entry 0 and [1, 37, 77, 77] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[INFO]: Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     train_epoch_loss, train_epoch_acc \u001b[39m=\u001b[39m train(swin_model_small, train_loader, \n\u001b[1;32m     19\u001b[0m                                             optimizer, criterion, device)\n\u001b[1;32m     20\u001b[0m     valid_epoch_loss, valid_epoch_acc \u001b[39m=\u001b[39m validate(swin_model_small, val_loader,  \n\u001b[1;32m     21\u001b[0m                                                 criterion, device)\n\u001b[1;32m     22\u001b[0m     writer\u001b[39m.\u001b[39madd_scalars(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39mfloat\u001b[39m(train_epoch_loss), \u001b[39m\"\u001b[39m\u001b[39mValidation\u001b[39m\u001b[39m\"\u001b[39m: valid_epoch_loss}, epoch)\n",
      "File \u001b[0;32m~/workspace/mlmi-project/training.py:11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m train_running_correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     10\u001b[0m counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(trainloader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(trainloader)):\n\u001b[1;32m     12\u001b[0m     counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     13\u001b[0m     image, labels \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rcvd_idx]) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1324\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info\u001b[39m.\u001b[39mpop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rcvd_idx)[\u001b[39m1\u001b[39m]\n\u001b[0;32m-> 1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sven/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [1, 36, 77, 77] at entry 0 and [1, 37, 77, 77] at entry 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from training import train, validate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# training \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(swin_model_small.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(swin_model_small, train_loader, \n",
    "                                            optimizer, criterion, device)\n",
    "    valid_epoch_loss, valid_epoch_acc = validate(swin_model_small, val_loader,  \n",
    "                                                criterion, device)\n",
    "    writer.add_scalars(\"Loss\", {\"Training\" : float(train_epoch_loss), \"Validation\": valid_epoch_loss}, epoch)\n",
    "    writer.add_scalars(\"Accuracy\", {\"Training\" : train_epoch_acc, \"Validation\": valid_epoch_acc}, epoch)\n",
    "    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Swin-Transformer Classification Large Model (pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SwinVitPretrainedClassifier:\n\tMissing key(s) in state_dict: \"_conv.weight\", \"_conv.bias\", \"_fc0.weight\", \"_fc0.bias\", \"_fc1.weight\", \"_fc1.bias\". \n\tUnexpected key(s) in state_dict: \"encoder1.layer.conv1.conv.weight\", \"encoder1.layer.conv2.conv.weight\", \"encoder1.layer.conv3.conv.weight\", \"encoder2.layer.conv1.conv.weight\", \"encoder2.layer.conv2.conv.weight\", \"encoder3.layer.conv1.conv.weight\", \"encoder3.layer.conv2.conv.weight\", \"encoder4.layer.conv1.conv.weight\", \"encoder4.layer.conv2.conv.weight\", \"encoder10.layer.conv1.conv.weight\", \"encoder10.layer.conv2.conv.weight\", \"decoder5.transp_conv.conv.weight\", \"decoder5.conv_block.conv1.conv.weight\", \"decoder5.conv_block.conv2.conv.weight\", \"decoder5.conv_block.conv3.conv.weight\", \"decoder4.transp_conv.conv.weight\", \"decoder4.conv_block.conv1.conv.weight\", \"decoder4.conv_block.conv2.conv.weight\", \"decoder4.conv_block.conv3.conv.weight\", \"decoder3.transp_conv.conv.weight\", \"decoder3.conv_block.conv1.conv.weight\", \"decoder3.conv_block.conv2.conv.weight\", \"decoder3.conv_block.conv3.conv.weight\", \"decoder2.transp_conv.conv.weight\", \"decoder2.conv_block.conv1.conv.weight\", \"decoder2.conv_block.conv2.conv.weight\", \"decoder2.conv_block.conv3.conv.weight\", \"decoder1.transp_conv.conv.weight\", \"decoder1.conv_block.conv1.conv.weight\", \"decoder1.conv_block.conv2.conv.weight\", \"decoder1.conv_block.conv3.conv.weight\", \"out.conv.conv.weight\", \"out.conv.conv.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# state_dict_mod = {}\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# for key, value in pretrained_weights[\"state_dict\"].items():\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#     # some modification to be able to load the params correctly\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m# pretrained_weights[\"state_dict\"] = state_dict_mod\u001b[39;00m\n\u001b[1;32m     19\u001b[0m swin_pretrained_model \u001b[39m=\u001b[39m SwinVitPretrainedClassifier()\u001b[39m.\u001b[39mdouble()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 20\u001b[0m swin_pretrained_model\u001b[39m.\u001b[39;49mload_state_dict(pretrained_weights[\u001b[39m\"\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(swin_pretrained_model)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlmi/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SwinVitPretrainedClassifier:\n\tMissing key(s) in state_dict: \"_conv.weight\", \"_conv.bias\", \"_fc0.weight\", \"_fc0.bias\", \"_fc1.weight\", \"_fc1.bias\". \n\tUnexpected key(s) in state_dict: \"encoder1.layer.conv1.conv.weight\", \"encoder1.layer.conv2.conv.weight\", \"encoder1.layer.conv3.conv.weight\", \"encoder2.layer.conv1.conv.weight\", \"encoder2.layer.conv2.conv.weight\", \"encoder3.layer.conv1.conv.weight\", \"encoder3.layer.conv2.conv.weight\", \"encoder4.layer.conv1.conv.weight\", \"encoder4.layer.conv2.conv.weight\", \"encoder10.layer.conv1.conv.weight\", \"encoder10.layer.conv2.conv.weight\", \"decoder5.transp_conv.conv.weight\", \"decoder5.conv_block.conv1.conv.weight\", \"decoder5.conv_block.conv2.conv.weight\", \"decoder5.conv_block.conv3.conv.weight\", \"decoder4.transp_conv.conv.weight\", \"decoder4.conv_block.conv1.conv.weight\", \"decoder4.conv_block.conv2.conv.weight\", \"decoder4.conv_block.conv3.conv.weight\", \"decoder3.transp_conv.conv.weight\", \"decoder3.conv_block.conv1.conv.weight\", \"decoder3.conv_block.conv2.conv.weight\", \"decoder3.conv_block.conv3.conv.weight\", \"decoder2.transp_conv.conv.weight\", \"decoder2.conv_block.conv1.conv.weight\", \"decoder2.conv_block.conv2.conv.weight\", \"decoder2.conv_block.conv3.conv.weight\", \"decoder1.transp_conv.conv.weight\", \"decoder1.conv_block.conv1.conv.weight\", \"decoder1.conv_block.conv2.conv.weight\", \"decoder1.conv_block.conv3.conv.weight\", \"out.conv.conv.weight\", \"out.conv.conv.bias\". "
     ]
    }
   ],
   "source": [
    "from models import SwinVitPretrainedClassifier\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "pretrained_swin_weights_path = \"pretrained/fold4_f48_ep300_4gpu_dice0_9035/model.pt\"\n",
    "pretrained_weights = torch.load(pretrained_swin_weights_path)\n",
    "\n",
    "# state_dict_mod = {}\n",
    "# for key, value in pretrained_weights[\"state_dict\"].items():\n",
    "#     # some modification to be able to load the params correctly\n",
    "#     parts = key.split('.')\n",
    "#     parts[0] = 'module'\n",
    "#     new_key = '.'.join(parts)\n",
    "#     new_key = new_key.replace(\"linear\", \"fc\" )\n",
    "#     state_dict_mod[new_key] = value\n",
    "\n",
    "# pretrained_weights[\"state_dict\"] = state_dict_mod\n",
    "\n",
    "swin_pretrained_model = SwinVitPretrainedClassifier().double().to(device)\n",
    "swin_pretrained_model.load_state_dict(pretrained_weights[\"state_dict\"])\n",
    "print(swin_pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 30\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a19d4f23d08424c849087c363a7a11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3120370533044db9877e888288d2b706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "Training loss: 0.721, training acc: 50.000\n",
      "Validation loss: 0.718, validation acc: 25.000\n",
      "[INFO]: Epoch 2 of 30\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850983fd09814f9f9c364afe0a67b856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b86f4490034ab5a706a4a1f5aa23cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "Training loss: 0.697, training acc: 50.000\n",
      "Validation loss: 0.709, validation acc: 25.000\n",
      "[INFO]: Epoch 3 of 30\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2857d0db18d442f6b56d58acd18c50f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af0cb7ffc1e417c9a623fc24fd6e7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n",
      "Training loss: 0.696, training acc: 50.000\n",
      "Validation loss: 0.703, validation acc: 25.000\n",
      "[INFO]: Epoch 4 of 30\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93314a47665440aa30337795e2df1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 2, 3, 3])\n",
      "torch.Size([1, 768, 2, 3, 3])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[INFO]: Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     train_epoch_loss, train_epoch_acc \u001b[39m=\u001b[39m train(swin_pretrained_model, train_loader, \n\u001b[1;32m     19\u001b[0m                                             optimizer, criterion, device)\n\u001b[1;32m     20\u001b[0m     valid_epoch_loss, valid_epoch_acc \u001b[39m=\u001b[39m validate(swin_pretrained_model, val_loader,  \n\u001b[1;32m     21\u001b[0m                                                 criterion, device)\n\u001b[1;32m     22\u001b[0m     writer\u001b[39m.\u001b[39madd_scalars(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39mfloat\u001b[39m(train_epoch_loss), \u001b[39m\"\u001b[39m\u001b[39mValidation\u001b[39m\u001b[39m\"\u001b[39m: valid_epoch_loss}, epoch)\n",
      "File \u001b[0;32m~/workspace/mlmi-project/training.py:14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     13\u001b[0m image, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m---> 14\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     15\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from training import train, validate\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# training \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, swin_pretrained_model.parameters()), lr=1e-4, weight_decay=1e-5)\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(swin_pretrained_model, train_loader, \n",
    "                                            optimizer, criterion, device)\n",
    "    valid_epoch_loss, valid_epoch_acc = validate(swin_pretrained_model, val_loader,  \n",
    "                                                criterion, device)\n",
    "    writer.add_scalars(\"Loss\", {\"Training\" : float(train_epoch_loss), \"Validation\": valid_epoch_loss}, epoch)\n",
    "    writer.add_scalars(\"Accuracy\", {\"Training\" : train_epoch_acc, \"Validation\": valid_epoch_acc}, epoch)\n",
    "    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "\n",
    "writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ('mlmi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
